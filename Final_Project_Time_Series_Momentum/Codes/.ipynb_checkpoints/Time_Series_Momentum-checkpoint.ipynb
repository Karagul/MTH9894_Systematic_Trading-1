{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTH 9894 Systematic Trading\n",
    "\n",
    "\n",
    "### Time Series Momentum Strategy\n",
    "\n",
    "\n",
    "#### Paper: Timer Series Momentum, Tobias, Yao, Lasse, 2012\n",
    "\n",
    "* Author: Hongchao Pan, Yu Sun\n",
    "* Kernel Version: Python 3.5\n",
    "* Packages: arch, numpy, stats\n",
    "* Data:\n",
    "* Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "1.      Import data and get the return\n",
    "\n",
    "2.      Exponentially weighted average return \\bar{r} in equation.1 {Maybe EWMA}\n",
    "\n",
    "3.      Compute ex ante annualized vol in equation.1\n",
    "\n",
    "4.      Run equation.2 (possibly equation 3 if time permitted) â€“ Then we have Fig.1 {How to derive monthly data?}\n",
    "\n",
    "5.      Compute r(k,h)\n",
    "\n",
    "6.      Run equation.4 ---- Get table.2\n",
    "\n",
    "7.      Run equation.5 ---- Get Figure.2\n",
    "\n",
    "\n",
    "10-12mins presentation, 3-5mins Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Codes/Get_data.py\n",
    "\"\"\"\n",
    "Copyright: Copyright (C) 2016 Baruch College - Systematic Trading\n",
    "Description: Functions to get data from data sets\n",
    "Author: Hongchao Pan, Yu Sun\n",
    "\"\"\"\n",
    "\n",
    "# Local imports\n",
    "\n",
    "# Load packages\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define a function to read the data for Equation 4 in the paper\n",
    "def df_eq4():\n",
    "    '''\n",
    "    Grasp the following data for equation 4: MKT, BOND, GSCI, SMB, HML, UMD\n",
    "    '''\n",
    "    MKT = pd.read_excel(io='../Data/MSCI_world.xlsx', sheetname=0, parse_cols='A:B', skiprows=4)\n",
    "    BOND = pd.read_excel(io='../Data/BarclaysBondIndex.xlsx', sheetname=0, parse_cols='A:B', skiprows=4)\n",
    "    GSCI = pd.read_excel(io='../Data/GSCI.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    SMBHML = pd.read_excel(io=\"../Data/F-F_Research_Data_Factors_daily.xlsx\", sheetname=0, parse_cols='F:H',\n",
    "                           skiprows=3)\n",
    "    UMD = pd.read_excel(io='../Data/F-F_Momentum_Factor_daily.xlsx', sheetname=0, parse_cols='C:D', skiprows=11)\n",
    "\n",
    "    # Merge the dateframes\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df = MKT.merge(BOND, on='Date', how='inner').dropna()\n",
    "    df = df.merge(GSCI, on='Date', how='inner').dropna()\n",
    "    df = df.merge(SMBHML, on='Date', how='inner').dropna()\n",
    "    df = df.merge(UMD, on='Date', how='inner').dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define a function to read all the bonds data\n",
    "def df_bonds():\n",
    "    '''\n",
    "    Get the 2Y, 5Y, 10Y, 30Y bonds data\n",
    "    :return: data frame contains all bonds data\n",
    "    '''\n",
    "\n",
    "    b2y=pd.read_excel(io='../Data/2ybond.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    b5y=pd.read_excel(io='../Data/5ybond.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    b10y = pd.read_excel(io='../Data/10ybond.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    b30y = pd.read_excel(io='../Data/30ybond.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df=b2y.merge(b5y, on='Date', how='inner').dropna()\n",
    "    df=df.merge(b10y, on='Date', how='inner').dropna()\n",
    "    df=df.merge(b30y, on='Date', how='inner').dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define a function to read currencies data\n",
    "def df_currency():\n",
    "    '''\n",
    "    EUR/USD, JPY/USD, GBP/USD\n",
    "    :return: data frame contains all data of selected currencies\n",
    "    '''\n",
    "\n",
    "    EU=pd.read_excel(io='../Data/EURUSD.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    JU=pd.read_excel(io='../Data/JPYUSDBOE.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    GU=pd.read_excel(io='../Data/GBPUSD.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df=EU.merge(JU, on='Date', how='inner').dropna()\n",
    "    df=df.merge(GU, on='Date', how='inner').dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define a function to read equity data (S&P500)\n",
    "def df_equity():\n",
    "    '''\n",
    "    S&P500, TOPIX(Japan), FTSE100(UK)\n",
    "    :return: data frame contains equity data\n",
    "    '''\n",
    "\n",
    "    SP500=pd.read_excel(io='../Data/SP500.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    TOPIX=pd.read_excel(io='../Data/TOPIX.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    FTSE100=pd.read_excel(io='../Data/FTSE100.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df=SP500.merge(TOPIX, on='Date', how='inner').dropna()\n",
    "    df=df.merge(FTSE100, on='Date', how='inner').dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define a function to read commodity data\n",
    "def df_commodity():\n",
    "    '''\n",
    "    Cotton, Sugar, NATGAS, CRUDE, GOLD, SILVER\n",
    "    :return: data frame contains commodity data\n",
    "    '''\n",
    "\n",
    "    cotton=pd.read_excel(io='../Data/cotton.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    sugar=pd.read_excel(io='../Data/sugar.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    natgas=pd.read_excel(io='../Data/natural_gas.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    crude=pd.read_excel(io='../Data/crude.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    gold=pd.read_excel(io='../Data/gold.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "    silver=pd.read_excel(io='../Data/silver.xlsx', sheetname=0, parse_cols='A:B', skiprows=1)\n",
    "\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df=cotton.merge(sugar, on='Date', how='inner').dropna()\n",
    "    df=df.merge(natgas, on='Date', how='inner').dropna()\n",
    "    df=df.merge(crude, on='Date', how='inner').dropna()\n",
    "    df=df.merge(gold, on='Date', how='inner').dropna()\n",
    "    df=df.merge(silver, on='Date', how='inner').dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define a function to combine all dataframes and separate the data to test_data and validation_data\n",
    "# Chose 10/27/1998-12/31/2012 as test_data\n",
    "# Chose 1/1/2013-12/31/2016 as validation_data\n",
    "\n",
    "def df_test_validation(Ts,Te, Vs,Ve):\n",
    "    '''\n",
    "    Test: 1998-2012\n",
    "    Validation: 2009-2016\n",
    "    :param Ts: Test start date with unit year\n",
    "    :param Te: Test end date with unit year\n",
    "    :param Vs: Validation start date with unit year\n",
    "    :param Ve: Validation end date with unit year\n",
    "    :return: test_data, validation_data\n",
    "    '''\n",
    "\n",
    "    eq4=df_eq4()\n",
    "    bonds=df_bonds()\n",
    "    equity=df_equity()\n",
    "    currency=df_currency()\n",
    "    commodity=df_commodity()\n",
    "\n",
    "    # Use dropna to drop all NaN elements when merge\n",
    "    df=eq4.merge(bonds, on='Date', how='inner').dropna()\n",
    "    df=df.merge(equity, on='Date', how='inner').dropna()\n",
    "    df=df.merge(currency, on='Date', how='inner').dropna()\n",
    "    df=df.merge(commodity, on='Date', how='inner').dropna()\n",
    "\n",
    "    # Add a new column with converted the \"Date\" to datetime objects\n",
    "    # for future slice\n",
    "    df['datetime']=[df.iloc[i,0].date() for i in range(len(df.iloc[:,0]))]\n",
    "\n",
    "    # Get the index of test data\n",
    "    ind_test=[Ts <= df.iloc[i]['datetime'].year <= Te for i in range(len(df.iloc[:, 0]))]\n",
    "    # Get the index of validation data\n",
    "    ind_validation=[Vs <= df.iloc[i]['datetime'].year <= Ve for i in range(len(df.iloc[:, 0]))]\n",
    "\n",
    "    # Get the test data\n",
    "    df_test=df[ind_test]\n",
    "    # Get the validation data\n",
    "    df_validation=df[ind_validation]\n",
    "\n",
    "    return df_test, df_validation\n",
    "\n",
    "# Define a function to read risk-free rates\n",
    "def df_rf():\n",
    "    '''\n",
    "    :return: risk-free rates\n",
    "    '''\n",
    "\n",
    "    df=pd.read_excel(io='../Data/RiskFreeRate.xlsx', sheetname=0, parse_cols='A:F', skiprows=0)[['Date','rf']]\n",
    "    # rf: is x.xx% not decimal format\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Codes/excess_return.py\n",
    "\"\"\"\n",
    "Copyright: Copyright (C) 2016 Baruch College - Systematic Trading\n",
    "Description: Functions to get data from data sets\n",
    "Author: Hongchao Pan, Yu Sun\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define a function to compute the excess return of test_data and validation_data\n",
    "def excess_return():\n",
    "    '''\n",
    "    excess return = percentage return - risk free rate\n",
    "    :return: excess return\n",
    "    '''\n",
    "\n",
    "    # Use pandas.pct_change to compute percentage of the test_data and validation_data\n",
    "    # Test start/end year\n",
    "    Ts = 1998\n",
    "    Te = 2012\n",
    "    # Validation start/end year\n",
    "    Vs = 2013\n",
    "    Ve = 2016\n",
    "\n",
    "    df_test, df_validation = df_test_validation(Ts, Te, Vs, Ve)\n",
    "\n",
    "    # Change the 'datetime' and 'Date' to the index for pct_change()\n",
    "    df_test.set_index(inplace=True, keys=['Date', 'datetime'])\n",
    "    df_test = df_test.pct_change()[1:]\n",
    "    df_test.reset_index(inplace=True)\n",
    "\n",
    "    df_validation.set_index(inplace=True, keys=['Date', 'datetime'])\n",
    "    df_validation = df_validation.pct_change()[1:]\n",
    "    df_validation.reset_index(inplace=True)\n",
    "\n",
    "    # Compute the excess return\n",
    "    df_rf=df_rf()\n",
    "    df_rf['rf']=df_rf['rf']/100 # Convert to decimal format\n",
    "    df_test=df_test.merge(df_rf, on='Date', how='inner').dropna()\n",
    "    df_validation=df_validation.merge(df_rf,on='Date', how='inner').dropna()\n",
    "\n",
    "    df_test_excess=df_test\n",
    "    df_validation_excess=df_test\n",
    "\n",
    "    for i in range(2,(len(df_test.columns)-1)):\n",
    "        df_test_excess.iloc[:,i]=df_test_excess.iloc[:,i]-df_test.iloc[:,(len(df_test.columns)-1)]\n",
    "        df_validation_excess.iloc[:, i] = df_validation_excess.iloc[:, i] - \\\n",
    "                                          df_validation.iloc[:, (len(df_test.columns) - 1)]\n",
    "\n",
    "\n",
    "    return df_test_excess, df_validation_excess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_rf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4c915bf9749b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_excess_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_excess_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcess_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-7728d7bad020>\u001b[0m in \u001b[0;36mexcess_return\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Compute the excess return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdf_rf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdf_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;31m# Convert to decimal format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df_rf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "df_excess_test, df_excess_validation=excess_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
